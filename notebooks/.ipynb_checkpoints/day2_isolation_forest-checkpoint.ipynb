{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Real ML Anomaly Detection with Isolation Forest\n",
    "\n",
    "Moving from statistical methods to machine learning for better anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "df = pd.read_csv(\"../data/cloud_cost_daily.csv\")\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values(\"date\")\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering\n",
    "\n",
    "Convert raw data into ML-ready features that capture time-series patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features (yesterday's cost affects today)\n",
    "df[\"cost_lag_1\"] = df[\"cost\"].shift(1)\n",
    "df[\"cost_lag_2\"] = df[\"cost\"].shift(2)\n",
    "\n",
    "# Rolling statistics (trend awareness)\n",
    "df[\"rolling_mean_3\"] = df[\"cost\"].rolling(3).mean()\n",
    "df[\"rolling_std_3\"] = df[\"cost\"].rolling(3).std()\n",
    "\n",
    "# Cost change rate\n",
    "df[\"cost_change\"] = df[\"cost\"].pct_change()\n",
    "\n",
    "# Remove rows with NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Features created. Shape after cleaning: {df.shape}\")\n",
    "df[[\"date\", \"cost\", \"cost_lag_1\", \"rolling_mean_3\", \"cost_change\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Isolation Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for ML model\n",
    "features = [\n",
    "    \"cost\",\n",
    "    \"cost_lag_1\", \n",
    "    \"cost_lag_2\",\n",
    "    \"rolling_mean_3\",\n",
    "    \"rolling_std_3\",\n",
    "    \"cost_change\"\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "\n",
    "# Train Isolation Forest\n",
    "model = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination=0.2,  # expect 20% anomalies\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Predict anomalies (-1 = anomaly, 1 = normal)\n",
    "df[\"anomaly_score\"] = model.fit_predict(X)\n",
    "df[\"is_anomaly\"] = df[\"anomaly_score\"] == -1\n",
    "\n",
    "print(f\"Anomalies detected: {df['is_anomaly'].sum()}\")\n",
    "print(f\"Anomaly rate: {df['is_anomaly'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visual Validation (CRITICAL FOR DEMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df[\"date\"], df[\"cost\"], label=\"Daily Cost\", linewidth=2)\n",
    "plt.scatter(\n",
    "    df[df[\"is_anomaly\"]][\"date\"],\n",
    "    df[df[\"is_anomaly\"]][\"cost\"],\n",
    "    color=\"red\",\n",
    "    s=100,\n",
    "    label=\"ML Detected Anomaly\",\n",
    "    zorder=5\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(\"Isolation Forest - Cloud Cost Anomaly Detection\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cost ($)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show detected anomalies\n",
    "print(\"\\nDetected Anomalies:\")\n",
    "df[df[\"is_anomaly\"]][[\"date\", \"cost\", \"cost_change\"]].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Ground Truth for Validation\n",
    "\n",
    "Since we don't have real labels, we simulate based on business logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ground truth: costs > $120 are true anomalies\n",
    "df[\"true_anomaly\"] = df[\"cost\"] > 120\n",
    "\n",
    "print(f\"True anomalies (cost > $120): {df['true_anomaly'].sum()}\")\n",
    "print(f\"ML detected anomalies: {df['is_anomaly'].sum()}\")\n",
    "\n",
    "# Show comparison\n",
    "comparison = df[[\"date\", \"cost\", \"true_anomaly\", \"is_anomaly\"]]\n",
    "comparison[comparison[\"true_anomaly\"] | comparison[\"is_anomaly\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Calculate Validation Metrics (HACKATHON REQUIREMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "precision = precision_score(df[\"true_anomaly\"], df[\"is_anomaly\"])\n",
    "recall = recall_score(df[\"true_anomaly\"], df[\"is_anomaly\"])\n",
    "f1 = f1_score(df[\"true_anomaly\"], df[\"is_anomaly\"])\n",
    "\n",
    "print(\"=== MODEL PERFORMANCE METRICS ===\")\n",
    "print(f\"Precision: {precision:.3f} (How many alerts were correct)\")\n",
    "print(f\"Recall: {recall:.3f} (How many true spikes we caught)\")\n",
    "print(f\"F1-Score: {f1:.3f} (Overall balance)\")\n",
    "\n",
    "# Confusion matrix breakdown\n",
    "true_positives = ((df[\"true_anomaly\"]) & (df[\"is_anomaly\"])).sum()\n",
    "false_positives = ((~df[\"true_anomaly\"]) & (df[\"is_anomaly\"])).sum()\n",
    "false_negatives = ((df[\"true_anomaly\"]) & (~df[\"is_anomaly\"])).sum()\n",
    "\n",
    "print(f\"\\n=== DETAILED BREAKDOWN ===\")\n",
    "print(f\"True Positives: {true_positives} (Correctly caught spikes)\")\n",
    "print(f\"False Positives: {false_positives} (False alarms)\")\n",
    "print(f\"False Negatives: {false_negatives} (Missed spikes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Business Impact Analysis (CRITICAL FOR JUDGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate business metrics\n",
    "total_anomaly_cost = df[df[\"is_anomaly\"]][\"cost\"].sum()\n",
    "avg_anomaly_cost = df[df[\"is_anomaly\"]][\"cost\"].mean()\n",
    "avg_normal_cost = df[~df[\"is_anomaly\"]][\"cost\"].mean()\n",
    "\n",
    "# Estimate potential savings (assume 30% of anomaly cost is preventable)\n",
    "estimated_monthly_savings = total_anomaly_cost * 0.3\n",
    "cost_increase_factor = avg_anomaly_cost / avg_normal_cost if avg_normal_cost > 0 else 0\n",
    "\n",
    "print(\"=== BUSINESS IMPACT ANALYSIS ===\")\n",
    "print(f\"Total anomalous spending detected: ${total_anomaly_cost:.2f}\")\n",
    "print(f\"Average anomaly cost: ${avg_anomaly_cost:.2f}\")\n",
    "print(f\"Average normal cost: ${avg_normal_cost:.2f}\")\n",
    "print(f\"Cost increase factor: {cost_increase_factor:.1f}x\")\n",
    "print(f\"\\nðŸ’° ESTIMATED MONTHLY SAVINGS: ${estimated_monthly_savings:.2f}\")\n",
    "print(f\"ðŸ“Š ROI: Early detection prevents {estimated_monthly_savings/total_anomaly_cost:.1%} of anomaly costs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Model Comparison (Baseline vs ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with Day 1 baseline (z-score method)\n",
    "df[\"z_score\"] = (df[\"cost\"] - df[\"rolling_mean_3\"]) / df[\"rolling_std_3\"]\n",
    "df[\"baseline_anomaly\"] = df[\"z_score\"].abs() > 2\n",
    "\n",
    "# Baseline metrics\n",
    "baseline_precision = precision_score(df[\"true_anomaly\"], df[\"baseline_anomaly\"])\n",
    "baseline_recall = recall_score(df[\"true_anomaly\"], df[\"baseline_anomaly\"])\n",
    "baseline_f1 = f1_score(df[\"true_anomaly\"], df[\"baseline_anomaly\"])\n",
    "\n",
    "print(\"=== METHOD COMPARISON ===\")\n",
    "print(f\"{'Metric':<12} {'Baseline (Z-Score)':<18} {'ML (Isolation Forest)':<20} {'Improvement':<12}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Precision':<12} {baseline_precision:<18.3f} {precision:<20.3f} {precision/baseline_precision if baseline_precision > 0 else 'N/A':<12}\")\n",
    "print(f\"{'Recall':<12} {baseline_recall:<18.3f} {recall:<20.3f} {recall/baseline_recall if baseline_recall > 0 else 'N/A':<12}\")\n",
    "print(f\"{'F1-Score':<12} {baseline_f1:<18.3f} {f1:<20.3f} {f1/baseline_f1 if baseline_f1 > 0 else 'N/A':<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Model for API (Day 3 Preparation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "# Save trained model\n",
    "joblib.dump(model, \"../models/isolation_forest_model.pkl\")\n",
    "\n",
    "# Save feature names for API\n",
    "with open(\"../models/feature_names.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(features))\n",
    "\n",
    "print(\"âœ… Model saved for API deployment\")\n",
    "print(f\"Model file: ../models/isolation_forest_model.pkl\")\n",
    "print(f\"Features: {features}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}